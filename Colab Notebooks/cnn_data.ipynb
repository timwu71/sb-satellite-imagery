{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEh5ojihBZ5_","executionInfo":{"status":"ok","timestamp":1651993000978,"user_tz":420,"elapsed":27564,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}},"outputId":"42391861-ca00-4cea-f990-cbdcb5195ff8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Enter the foldername for the data set (added shortcut to 230/231N folder)\n","FOLDERNAME = 'Shareddrives/CS 230 231N/public_datasets'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\""],"metadata":{"id":"xQQ9TdrvBdKh","executionInfo":{"status":"ok","timestamp":1651993074733,"user_tz":420,"elapsed":184,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FDGmVQOOAGa-","executionInfo":{"status":"ok","timestamp":1651993076319,"user_tz":420,"elapsed":153,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}}},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","source":["import sys\n","dataset_path = '/content/drive/My Drive/{}'.format(FOLDERNAME)\n","# sys.path.append(dataset_path)"],"metadata":{"id":"ALWMS73sCxkr","executionInfo":{"status":"ok","timestamp":1651993080277,"user_tz":420,"elapsed":188,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from concurrent.futures import ThreadPoolExecutor\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","from tqdm.auto import tqdm"],"metadata":{"id":"qugBs-GCAUON","executionInfo":{"status":"ok","timestamp":1651993085219,"user_tz":420,"elapsed":817,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# dataset_root_dir = '/content/drive/Shareddrives/CS 230 231N/sustainbench-main/sustainbench-main/dataset_preprocessing/dhs_lsms'"],"metadata":{"id":"wtwh_XP2AUm9","executionInfo":{"status":"ok","timestamp":1651993098302,"user_tz":420,"elapsed":214,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# TODO: copy dhs_final_labels.csv to VM, change os path below accordingly\n","df = pd.read_csv(os.path.join(dataset_root_dir, 'output_labels/dhs_final_labels.csv'))\n","df['survey'] = df['DHSID_EA'].str[:10]\n","df['cc'] = df['DHSID_EA'].str[:2]\n","\n","# TODO: run modified version of get_public_datasets.py, change data_dir below to match VM path\n","data_dir = '/content/drive/Shareddrives/CS 230 231N/dhs_datasets/'\n","df['path'] = data_dir + df['survey'] + '/' + df['DHSID_EA'] + '.npz'\n","# df['path'] = dataset_root_dir + '/dhs_npzs/' + df['survey'] + '/' + df['DHSID_EA'] + '.npz'\n","\n","path_years = df[['DHSID_EA', 'path', 'year']].apply(tuple, axis=1)\n","df.set_index('DHSID_EA', verify_integrity=True, inplace=True, drop=False) #had to add drop=False to keep column from disappearing  -- R\n","print(df['path'].iloc[0])\n","df.info()\n","display(df.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k_iVBcDmAVO6","executionInfo":{"status":"ok","timestamp":1651993108280,"user_tz":420,"elapsed":4604,"user":{"displayName":"Shayana Aarthy Venukanthan","userId":"03640355451445516907"}},"outputId":"386d1fb1-b9ee-4142-9742-14d70cded87b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/CS 230 231N/dhs_datasets/AL-2008-5#/AL-2008-5#-00000001.npz\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 117644 entries, AL-2008-5#-00000001 to ZW-2015-7#-00000400\n","Data columns (total 24 columns):\n"," #   Column            Non-Null Count   Dtype  \n","---  ------            --------------   -----  \n"," 0   DHSID_EA          117644 non-null  object \n"," 1   cname             117644 non-null  object \n"," 2   year              117644 non-null  int64  \n"," 3   lat               117644 non-null  float64\n"," 4   lon               117644 non-null  float64\n"," 5   n_asset           86936 non-null   float64\n"," 6   asset_index       86936 non-null   float64\n"," 7   n_water           87938 non-null   float64\n"," 8   water_index       87938 non-null   float64\n"," 9   n_sanitation      89271 non-null   float64\n"," 10  sanitation_index  89271 non-null   float64\n"," 11  under5_mort       105582 non-null  float64\n"," 12  n_under5_mort     105582 non-null  float64\n"," 13  women_edu         117062 non-null  float64\n"," 14  women_bmi         94866 non-null   float64\n"," 15  n_women_edu       117062 non-null  float64\n"," 16  n_women_bmi       94866 non-null   float64\n"," 17  cluster_id        117644 non-null  int64  \n"," 18  adm1fips          45916 non-null   object \n"," 19  adm1dhs           117644 non-null  int64  \n"," 20  urban             117644 non-null  object \n"," 21  survey            117644 non-null  object \n"," 22  cc                117644 non-null  object \n"," 23  path              117644 non-null  object \n","dtypes: float64(14), int64(3), object(7)\n","memory usage: 22.4+ MB\n"]},{"output_type":"display_data","data":{"text/plain":["                                DHSID_EA cname  year        lat        lon  \\\n","DHSID_EA                                                                     \n","AL-2008-5#-00000001  AL-2008-5#-00000001    AL  2008  40.822652  19.838321   \n","AL-2008-5#-00000002  AL-2008-5#-00000002    AL  2008  40.696846  20.007555   \n","AL-2008-5#-00000003  AL-2008-5#-00000003    AL  2008  40.750037  19.974262   \n","AL-2008-5#-00000004  AL-2008-5#-00000004    AL  2008  40.798931  19.863338   \n","AL-2008-5#-00000005  AL-2008-5#-00000005    AL  2008  40.746123  19.843885   \n","\n","                     n_asset  asset_index  n_water  water_index  n_sanitation  \\\n","DHSID_EA                                                                        \n","AL-2008-5#-00000001     18.0     2.430596     18.0     3.444444          18.0   \n","AL-2008-5#-00000002     20.0     2.867678     20.0     4.700000          20.0   \n","AL-2008-5#-00000003     18.0     2.909049     18.0     4.500000          18.0   \n","AL-2008-5#-00000004     19.0     2.881122     19.0     4.947368          19.0   \n","AL-2008-5#-00000005     19.0     2.546830     19.0     4.684211          19.0   \n","\n","                     ...  women_bmi  n_women_edu  n_women_bmi  cluster_id  \\\n","DHSID_EA             ...                                                    \n","AL-2008-5#-00000001  ...  24.365000         18.0         18.0           1   \n","AL-2008-5#-00000002  ...  23.104000         20.0         20.0           2   \n","AL-2008-5#-00000003  ...  22.387778         18.0         18.0           3   \n","AL-2008-5#-00000004  ...  27.084500         21.0         20.0           4   \n","AL-2008-5#-00000005  ...  24.523125         16.0         16.0           5   \n","\n","                     adm1fips  adm1dhs  urban      survey  cc  \\\n","DHSID_EA                                                        \n","AL-2008-5#-00000001       NaN     9999      R  AL-2008-5#  AL   \n","AL-2008-5#-00000002       NaN     9999      R  AL-2008-5#  AL   \n","AL-2008-5#-00000003       NaN     9999      R  AL-2008-5#  AL   \n","AL-2008-5#-00000004       NaN     9999      R  AL-2008-5#  AL   \n","AL-2008-5#-00000005       NaN     9999      R  AL-2008-5#  AL   \n","\n","                                                                  path  \n","DHSID_EA                                                                \n","AL-2008-5#-00000001  /content/drive/Shareddrives/CS 230 231N/dhs_da...  \n","AL-2008-5#-00000002  /content/drive/Shareddrives/CS 230 231N/dhs_da...  \n","AL-2008-5#-00000003  /content/drive/Shareddrives/CS 230 231N/dhs_da...  \n","AL-2008-5#-00000004  /content/drive/Shareddrives/CS 230 231N/dhs_da...  \n","AL-2008-5#-00000005  /content/drive/Shareddrives/CS 230 231N/dhs_da...  \n","\n","[5 rows x 24 columns]"],"text/html":["\n","  <div id=\"df-b05d367d-2e12-41a9-95a4-f39ffbadb979\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DHSID_EA</th>\n","      <th>cname</th>\n","      <th>year</th>\n","      <th>lat</th>\n","      <th>lon</th>\n","      <th>n_asset</th>\n","      <th>asset_index</th>\n","      <th>n_water</th>\n","      <th>water_index</th>\n","      <th>n_sanitation</th>\n","      <th>...</th>\n","      <th>women_bmi</th>\n","      <th>n_women_edu</th>\n","      <th>n_women_bmi</th>\n","      <th>cluster_id</th>\n","      <th>adm1fips</th>\n","      <th>adm1dhs</th>\n","      <th>urban</th>\n","      <th>survey</th>\n","      <th>cc</th>\n","      <th>path</th>\n","    </tr>\n","    <tr>\n","      <th>DHSID_EA</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>AL-2008-5#-00000001</th>\n","      <td>AL-2008-5#-00000001</td>\n","      <td>AL</td>\n","      <td>2008</td>\n","      <td>40.822652</td>\n","      <td>19.838321</td>\n","      <td>18.0</td>\n","      <td>2.430596</td>\n","      <td>18.0</td>\n","      <td>3.444444</td>\n","      <td>18.0</td>\n","      <td>...</td>\n","      <td>24.365000</td>\n","      <td>18.0</td>\n","      <td>18.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>9999</td>\n","      <td>R</td>\n","      <td>AL-2008-5#</td>\n","      <td>AL</td>\n","      <td>/content/drive/Shareddrives/CS 230 231N/dhs_da...</td>\n","    </tr>\n","    <tr>\n","      <th>AL-2008-5#-00000002</th>\n","      <td>AL-2008-5#-00000002</td>\n","      <td>AL</td>\n","      <td>2008</td>\n","      <td>40.696846</td>\n","      <td>20.007555</td>\n","      <td>20.0</td>\n","      <td>2.867678</td>\n","      <td>20.0</td>\n","      <td>4.700000</td>\n","      <td>20.0</td>\n","      <td>...</td>\n","      <td>23.104000</td>\n","      <td>20.0</td>\n","      <td>20.0</td>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>9999</td>\n","      <td>R</td>\n","      <td>AL-2008-5#</td>\n","      <td>AL</td>\n","      <td>/content/drive/Shareddrives/CS 230 231N/dhs_da...</td>\n","    </tr>\n","    <tr>\n","      <th>AL-2008-5#-00000003</th>\n","      <td>AL-2008-5#-00000003</td>\n","      <td>AL</td>\n","      <td>2008</td>\n","      <td>40.750037</td>\n","      <td>19.974262</td>\n","      <td>18.0</td>\n","      <td>2.909049</td>\n","      <td>18.0</td>\n","      <td>4.500000</td>\n","      <td>18.0</td>\n","      <td>...</td>\n","      <td>22.387778</td>\n","      <td>18.0</td>\n","      <td>18.0</td>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>9999</td>\n","      <td>R</td>\n","      <td>AL-2008-5#</td>\n","      <td>AL</td>\n","      <td>/content/drive/Shareddrives/CS 230 231N/dhs_da...</td>\n","    </tr>\n","    <tr>\n","      <th>AL-2008-5#-00000004</th>\n","      <td>AL-2008-5#-00000004</td>\n","      <td>AL</td>\n","      <td>2008</td>\n","      <td>40.798931</td>\n","      <td>19.863338</td>\n","      <td>19.0</td>\n","      <td>2.881122</td>\n","      <td>19.0</td>\n","      <td>4.947368</td>\n","      <td>19.0</td>\n","      <td>...</td>\n","      <td>27.084500</td>\n","      <td>21.0</td>\n","      <td>20.0</td>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>9999</td>\n","      <td>R</td>\n","      <td>AL-2008-5#</td>\n","      <td>AL</td>\n","      <td>/content/drive/Shareddrives/CS 230 231N/dhs_da...</td>\n","    </tr>\n","    <tr>\n","      <th>AL-2008-5#-00000005</th>\n","      <td>AL-2008-5#-00000005</td>\n","      <td>AL</td>\n","      <td>2008</td>\n","      <td>40.746123</td>\n","      <td>19.843885</td>\n","      <td>19.0</td>\n","      <td>2.546830</td>\n","      <td>19.0</td>\n","      <td>4.684211</td>\n","      <td>19.0</td>\n","      <td>...</td>\n","      <td>24.523125</td>\n","      <td>16.0</td>\n","      <td>16.0</td>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>9999</td>\n","      <td>R</td>\n","      <td>AL-2008-5#</td>\n","      <td>AL</td>\n","      <td>/content/drive/Shareddrives/CS 230 231N/dhs_da...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 24 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b05d367d-2e12-41a9-95a4-f39ffbadb979')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b05d367d-2e12-41a9-95a4-f39ffbadb979 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b05d367d-2e12-41a9-95a4-f39ffbadb979');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"markdown","source":["Assuming we are connected to a VM that has all the data and has already run a modified version of get_public_datasets.py that unzipped files from \"data\" folder and placed them in the \"dhs_datasets\" folder:"],"metadata":{"id":"wRwaqreYeYwn"}},{"cell_type":"code","source":["# Function: paths_to_X\n","# build input matrix X by iterating through each path in paths, load image, insert in X\n","def paths_to_X(paths):  # -> (N, C, H, W) model input X\n","  '''\n","    Args\n","    - paths: array (N, 1)\n","      - path: str, path to npz file containing single entry 'x'\n","        representing a (C, H, W) image\n","\n","    Returns: X, input matrix (N, C, H, W)\n","    '''\n","  N = len(paths)  # should be 117644\n","  C, H, W = 8, 255, 255\n","  X = np.zeros((N, C, H, W))\n","  for n in range(N):\n","    npz_path = paths[n]\n","    img = np.load(npz_path)['x']  # shape (C, H, W)\n","    X[n, :, :, :] = img\n","  return X"],"metadata":{"id":"oxJQ7Uk8eXEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# label_cols = ['asset_index', 'under5_mort', 'women_bmi', 'women_edu', 'water_index', 'sanitation_index']\n","# label_cols = ['n_under5_mort']\n","label = \"n_under5_mort\""],"metadata":{"id":"5J-qvpNeAcaB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SPLITS = {\n","    'train': [\n","        'AL', 'BD', 'CD', 'CM', 'GH', 'GU', 'HN', 'IA', 'ID', 'JO', 'KE', 'KM',\n","        'LB', 'LS', 'MA', 'MB', 'MD', 'MM', 'MW', 'MZ', 'NG', 'NI', 'PE', 'PH',\n","        'SN', 'TG', 'TJ', 'UG', 'ZM', 'ZW'],\n","    'val': [\n","        'BF', 'BJ', 'BO', 'CO', 'DR', 'GA', 'GN', 'GY', 'HT', 'NM', 'SL', 'TD',\n","        'TZ'],\n","    'test': [\n","        'AM', 'AO', 'BU', 'CI', 'EG', 'ET', 'KH', 'KY', 'ML', 'NP', 'PK', 'RW',\n","        'SZ']\n","}\n","SPLITS['trainval'] = SPLITS['train'] + SPLITS['val']"],"metadata":{"id":"2NJqCig9AloV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_train_and_test(label, trainsplit='train', testsplit='test'):\n","    train_dhsids = df.index[df['cc'].isin(SPLITS[trainsplit]) & df[label].notna()]\n","    test_dhsids = df.index[df['cc'].isin(SPLITS[testsplit]) & df[label].notna()]\n","\n","    train_X_paths = df.loc[train_dhsids, 'path'].values.reshape(-1, 1)\n","    train_X = paths_to_X(train_X_paths)  # (N, C, H, W)\n","    train_Y = df.loc[train_dhsids, label].values \n","    test_X_paths = df.loc[test_dhsids, 'path'].values.reshape(-1, 1)\n","    test_X = paths_to_X(test_X_paths)\n","    test_Y = df.loc[test_dhsids, label].values\n","\n","    # knn.fit(train_X, train_Y)\n","    # preds = knn.predict(test_X)\n","    return train_X, train_Y, test_X, test_Y"],"metadata":{"id":"3RKf2DuNl9NO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_X, train_Y, val_X, val_Y = get_train_and_test(label, 'train', 'val') \n","trainval_X, trainval_Y, test_X, test_Y = get_train_and_test(label, 'trainval', 'test')\n","print(\"train_X: \", train_X.shape)\n","print(\"train_Y: \", train_Y.shape)\n","print(\"trainval_X: \", trainval_X.shape)\n","print(\"trainval_Y: \", trainval_Y.shape)\n","print(\"val_X: \", val_X.shape)\n","print(\"val_Y: \", val_Y.shape)\n","print(\"test_X: \", test_X.shape)\n","print(\"test_Y: \", test_Y.shape)"],"metadata":{"id":"vIGezxheuXyU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# IGNORE AFTER THIS - NOT DONE\n"],"metadata":{"id":"joQLB8fLukM0"}},{"cell_type":"code","source":["# From assignment2 PyTorch.ipynb\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","\n","import torch.nn.functional as F  # useful stateless functions\n","\n","import numpy as np\n","\n","USE_GPU = True\n","dtype = torch.float32 # We will be using float throughout this tutorial.\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","# Constant to control how frequently we print train loss.\n","print_every = 100\n","print('using device:', device)"],"metadata":{"id":"RQ26hn7SApCU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From assignment2 PyTorch.ipynb\n","\n","# TODO: find replacement for loader - make function make minibatches\n","\n","def check_accuracy_part34(X, Y, model, val_or_test):\n","    if val_or_test == \"val\":\n","        print('Checking accuracy on validation set')\n","    elif val_or_test == \"test\":\n","        print('Checking accuracy on test set')\n","\n","    batch_size = 64\n","    num_batches = Y.shape // batch_size   \n","    num_correct = 0\n","    num_samples = 0\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for t in range(num_batches):\n","          x = X[t*num_batches:(t+1)*num_batches, :, :, :]\n","          y = Y[t*num_batches:(t+1)*num_batches]\n","          x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","          y = y.to(device=device, dtype=torch.long)\n","          scores = model(x)\n","          _, preds = scores.max(1)\n","          num_correct += (preds == y).sum()\n","          num_samples += preds.size(0)\n","        acc = float(num_correct) / num_samples\n","        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n","    return acc"],"metadata":{"id":"HY9KYIShsyo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# From assignment2 PyTorch.ipynb\n","\n","def train_part34(model, optimizer, val_or_test, epochs=1):\n","    \"\"\"\n","    Train a model using the PyTorch Module API.\n","    \n","    Inputs:\n","    - model: A PyTorch Module giving the model to train.\n","    - optimizer: An Optimizer object we will use to train the model\n","    - epochs: (Optional) A Python integer giving the number of epochs to train for\n","    \n","    Returns: Nothing, but prints model accuracies during training.\n","    \"\"\"\n","    \n","    batch_size = 64\n","    if val_or_test == \"val\":\n","      X = train_X\n","      Y = train_Y\n","    elif val_or_test == \"test\":\n","      X = trainval_X\n","      Y = trainval_Y\n","    num_batches = Y.shape // batch_size\n","\n","    model = model.to(device=device)  # move the model parameters to CPU/GPU\n","    for e in range(epochs):\n","      # TODO: find replacement for loader to make minibatches x, y from X, Y\n","        for t in range(num_batches):\n","          x = X[t*num_batches:(t+1)*num_batches, :, :, :]\n","          y = Y[t*num_batches:(t+1)*num_batches]\n","          model.train()  # put model to training mode\n","          x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n","          y = y.to(device=device, dtype=torch.long)\n","\n","          scores = model(x)\n","          loss = F.cross_entropy(scores, y)\n","\n","          # Zero out all of the gradients for the variables which the optimizer\n","          # will update.\n","          optimizer.zero_grad()\n","\n","          # This is the backwards pass: compute the gradient of the loss with\n","          # respect to each  parameter of the model.\n","          loss.backward()\n","\n","          # Actually update the parameters of the model using the gradients\n","          # computed by the backwards pass.\n","          optimizer.step()\n","\n","          if t % print_every == 0:\n","              print('Iteration %d, loss = %.4f' % (t, loss.item()))\n","              check_accuracy_part34(X, Y, model, \"val\")\n","              print()\n"],"metadata":{"id":"tglOer9aLgVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def flatten(x):\n","    N = x.shape[0] # read in N, C, H, W\n","    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n","\n","# We need to wrap `flatten` function in a module in order to stack it\n","# in nn.Sequential\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return flatten(x)"],"metadata":{"id":"xXIq0oBIDYUd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_val = 0"],"metadata":{"id":"whwiH1DQFT5W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = None\n","optimizer = None\n","\n","channel_0 = 8\n","channel_1 = 32\n","channel_2 = 16\n","channel_3 = 16\n","hidden_layer_size = 32\n","learning_rate = 1e-2\n","\n","model = nn.Sequential(\n","    nn.Conv2d(channel_0, channel_1, (3, 3), padding=\"same\"),\n","    nn.ReLU(),\n","    nn.MaxPool2d((2, 2), stride=2),  # changes H, W from 32 to 16\n","    nn.BatchNorm2d(num_features = channel_1),\n","    nn.Conv2d(channel_1, channel_2, (3, 3), padding=\"same\"),\n","    nn.ReLU(),\n","    nn.MaxPool2d((2, 2), stride=2),  # changes H, W from 16 to 8\n","    nn.BatchNorm2d(num_features = channel_2),\n","    nn.Conv2d(channel_2, channel_3, (3, 3), padding=\"same\"),\n","    nn.ReLU(),\n","    nn.MaxPool2d((2, 2), stride=2),  # changes H, W from 8 to 4\n","    nn.BatchNorm2d(num_features = channel_3),\n","    Flatten(),\n","    nn.Linear(channel_2 * 4 * 4, hidden_layer_size),\n","    nn.ReLU(),\n","    nn.Linear(hidden_layer_size, 10),\n",")\n","\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n","\n","\n","train_part34(model, optimizer, epochs=10, val_or_test=\"val\")\n","val_acc = check_accuracy_part34(val_X, val_Y, model, \"val\")\n","if val_acc > best_val:\n","  best_model = model"],"metadata":{"id":"9SIdNqU7CtUf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E89Ezf06HAyb"},"source":["## Test set -- run this only once\n","\n","Now that we've gotten a result we're happy with, we test our final model on the test set (which you should store in best_model). Think about how this compares to your validation set accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwvXcgBRHAyb","executionInfo":{"status":"ok","timestamp":1651650503018,"user_tz":420,"elapsed":3699,"user":{"displayName":"Shayana Venukanthan","userId":"12375484925168942718"}},"outputId":"27f8c55f-1363-4a24-a544-3c18a67eaa80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking accuracy on test set\n","Got 7236 / 10000 correct (72.36)\n"]}],"source":["best_model = model\n","check_accuracy_part34(test_X, test_Y, best_model, \"test\")"]}]}